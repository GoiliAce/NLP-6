{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://vnexpress.net'\n",
    "headers = ['the-thao', 'khoa-hoc', 'suc-khoe', 'giao-duc', 'du-lich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_subpage =[]\n",
    "for header in headers:\n",
    "    url_post = []\n",
    "    \n",
    "    for page in range(1,10):\n",
    "        response = requests.get(url_base+'/'+header+'-p'+str(page))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        posts = soup.find_all(class_='title-news')\n",
    "        for post in posts:\n",
    "            url_post.append(post.a['href'])\n",
    "    url_subpage.append({\n",
    "        header: list(set(url_post))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url.json', 'w') as f:\n",
    "    json.dump(url_subpage, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_subpage[1]['khoa-hoc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -r data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./data')\n",
    "# for header in headers:\n",
    "#   os.makedirs(f'./data/{header}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the-thao - 132:   0%|          | 0/5 [01:17<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the-thao - 143:   0%|          | 0/5 [01:24<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the-thao - 149:   0%|          | 0/5 [01:28<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the-thao - 212:   0%|          | 0/5 [02:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "khoa-hoc - 25:  20%|██        | 1/5 [02:38<09:22, 140.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "giao-duc - 86:  60%|██████    | 3/5 [08:37<05:06, 153.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "giao-duc - 106:  60%|██████    | 3/5 [08:51<05:06, 153.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "giao-duc - 138:  60%|██████    | 3/5 [09:13<05:06, 153.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 18:  80%|████████  | 4/5 [10:41<02:40, 160.40s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 20:  80%|████████  | 4/5 [10:43<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 31:  80%|████████  | 4/5 [10:53<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 57:  80%|████████  | 4/5 [11:11<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 59:  80%|████████  | 4/5 [11:13<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 80:  80%|████████  | 4/5 [11:28<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 103:  80%|████████  | 4/5 [11:44<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 113:  80%|████████  | 4/5 [11:52<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 127:  80%|████████  | 4/5 [12:03<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 134:  80%|████████  | 4/5 [12:08<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 134:  80%|████████  | 4/5 [12:08<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 166:  80%|████████  | 4/5 [12:30<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 193:  80%|████████  | 4/5 [12:50<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 230:  80%|████████  | 4/5 [13:15<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 240:  80%|████████  | 4/5 [13:24<02:40, 160.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "du-lich - 249: 100%|██████████| 5/5 [13:31<00:00, 162.38s/it]\n"
     ]
    }
   ],
   "source": [
    "rag = tqdm(url_subpage)\n",
    "for urls in rag:\n",
    "    header =list(urls.keys())[0]\n",
    "    url_post = list(urls.values())[0]\n",
    "    json_result = []\n",
    "    while len(json_result) <250 and url_post:\n",
    "        rag.set_description(f'{header} - {len(json_result)}')\n",
    "        url = url_post.pop()\n",
    "        response = requests.get(url)\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.find(class_='title-detail').text\n",
    "            description = soup.find('p', class_='description')\n",
    "            for i in description.find_all(class_='location-stamp'):\n",
    "                i.decompose()\n",
    "            description = description.text\n",
    "            content = '\\n'.join([p.text for p in soup.find_all('p', class_='Normal')])\n",
    "            title_ = re.sub(r'[^a-zA-Z]+', '', title).lower()\n",
    "            content = description+'\\n'+content\n",
    "            if len(content) < 1000:\n",
    "                continue\n",
    "            json_result.append({\n",
    "                'title': title,\n",
    "                'content': description+'\\n'+content,\n",
    "            })\n",
    "        except AttributeError:\n",
    "            print('Error')\n",
    "            continue\n",
    "        \n",
    "    with open(f'./data/{header}.json', 'w') as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tách từ (Tokenize) sử dụng thư viện pyvi hay underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./data')\n",
    "paths = [file for file in files if file.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for path in paths:\n",
    "    with open( os.path.join('./data', path), 'r') as f:\n",
    "        doc = json.load(f)\n",
    "        for item in doc:\n",
    "            data.append(item['content'])\n",
    "            labels.append(path.split('.')[0])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'data': data, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['data'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: word_tokenize(x, format=\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Lấy danh sách stop words tiếng Việt\n",
    "# stop_words = set(stopwords.words('vietnamese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process(text):\n",
    "#     text = word_tokenize(text, format=\"text\")\n",
    "#     text = [word for word in text.split() if word not in stop_words]\n",
    "#     return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = df['text'].apply(lambda x: process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1235 entries, 0 to 1234\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   data    1235 non-null   object\n",
      " 1   label   1235 non-null   object\n",
      " 2   text    1235 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 29.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=.1, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5500)\n",
    "tfidf_vectorizer.fit(df['text'])\n",
    "\n",
    "X_train_idf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_idf = tfidf_vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn= KNeighborsClassifier(n_neighbors=10)\n",
    "kf= KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores= cross_val_score(knn, X=X_train_idf, y=y_train, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97757848 0.96396396 0.98648649 0.95495495 0.96846847]\n",
      "0.9702904698420394\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9758064516129032\n",
      "Confusion Matrix:\n",
      " [[25  0  2  0  0]\n",
      " [ 0 22  0  0  0]\n",
      " [ 0  0 21  0  0]\n",
      " [ 0  0  1 26  0]\n",
      " [ 0  0  0  0 27]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_idf.toarray(), y_train)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "y_pred = svm_model.predict(X_test_idf.toarray())\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9193548387096774\n",
      "Confusion Matrix:\n",
      " [[24  1  2  0  0]\n",
      " [ 3 19  0  0  0]\n",
      " [ 2  0 19  0  0]\n",
      " [ 0  1  0 26  0]\n",
      " [ 1  0  0  0 26]]\n"
     ]
    }
   ],
   "source": [
    "bayes_model = GaussianNB()\n",
    "bayes_model.fit(X_train_idf.toarray(), y_train)\n",
    "\n",
    "y_pred = bayes_model.predict(X_test_idf.toarray())\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProcessText(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, list):\n",
    "            return self.process(X)\n",
    "        return self.process([X])\n",
    "        return processed_text\n",
    "    def process(self, text):\n",
    "        for t in text:\n",
    "            t = t.lower()\n",
    "            t = word_tokenize(t, format=\"text\")\n",
    "        return tfidf_vectorizer.transform(text).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['giao-duc'], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_predict = Pipeline([\n",
    "    ('process', ProcessText()),\n",
    "    ('model', svm_model)\n",
    "])\n",
    "\n",
    "\n",
    "predicted_class = pipeline_predict.predict(\"\"\"Năm nay, hơn 660.000 thí sinh đăng ký xét tuyển đại học với 3,4 triệu nguyện vọng. Ngay sau khi kết thúc thời gian nhập học, các trường chưa tuyển đủ được tuyển bổ sung đến tháng 12.\n",
    "\"\"\")\n",
    "lb.inverse_transform(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline_predict, f)\n",
    "    \n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(lb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 340)\t-0.5773502691896258\n",
      "  (0, 397)\t-0.5773502691896258\n",
      "  (0, 451)\t-0.5773502691896258\n",
      "  (1, 282)\t0.5\n",
      "  (1, 340)\t-0.5\n",
      "  (1, 397)\t-0.5\n",
      "  (1, 451)\t-0.5\n",
      "  (2, 361)\t0.5\n",
      "  (2, 397)\t-0.5\n",
      "  (2, 448)\t0.5\n",
      "  (2, 451)\t-0.5\n",
      "  (3, 113)\t0.4472135954999579\n",
      "  (3, 340)\t-0.4472135954999579\n",
      "  (3, 352)\t0.4472135954999579\n",
      "  (3, 397)\t-0.4472135954999579\n",
      "  (3, 451)\t-0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    " ' Hôm_nay tôi đi_học', \n",
    " ' Hôm_nay tôi đi_học ở trường', \n",
    " ' Hôm_nay tôi nghỉ ở nhà', \n",
    " ' Hôm_nay tôi có đi_học không?',\n",
    "]\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Câu 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1235x999 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 264863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=999)\n",
    "vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hv = vectorizer.transform(X_train)\n",
    "X_test_hv = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9838709677419355\n",
      "Confusion Matrix:\n",
      " [[26  0  1  0  0]\n",
      " [ 0 22  0  0  0]\n",
      " [ 0  0 21  0  0]\n",
      " [ 0  0  0 27  0]\n",
      " [ 0  0  1  0 26]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_hv, y_train)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "y_pred = svm_model.predict(X_test_hv)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9112903225806451\n",
      "Confusion Matrix:\n",
      " [[23  0  4  0  0]\n",
      " [ 1 20  0  0  1]\n",
      " [ 1  1 17  1  1]\n",
      " [ 1  0  0 26  0]\n",
      " [ 0  0  0  0 27]]\n"
     ]
    }
   ],
   "source": [
    "bayes_model = GaussianNB()\n",
    "bayes_model.fit(X_train_hv.toarray(), y_train)\n",
    "\n",
    "y_pred = bayes_model.predict(X_test_hv.toarray())\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
